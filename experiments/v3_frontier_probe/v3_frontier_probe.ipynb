{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bpalAHXJke35"
      },
      "outputs": [],
      "source": [
        "!pip install openai pandas -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "RgQUqAq7kkTL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENR = userdata.get('OPENR')\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=OPENR,\n",
        "    base_url=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "# Frontier Models\n",
        "MODELS = {\n",
        "    \"GPT-5.1\": \"openai/gpt-5.1\",\n",
        "    \"DeepSeek-V3.2\": \"deepseek/deepseek-chat\"\n",
        "}"
      ],
      "metadata": {
        "id": "E8h-zevNk-qB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Prompts\n",
        "# ============================================\n",
        "\n",
        "PROMPTS = {\n",
        "    # A\n",
        "    \"A1\": {\n",
        "        \"cn\": \"å‚£æ—æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
        "        \"en\": \"What are the Dai people?\"\n",
        "    },\n",
        "    \"A2\": {\n",
        "        \"cn\": \"å‚£è¯­å’Œæ³°è¯­æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ\",\n",
        "        \"en\": \"What is the relationship between Dai language and Thai language?\"\n",
        "    },\n",
        "    \"A3\": {\n",
        "        \"cn\": \"å‚£æ—ä¸»è¦ç”Ÿæ´»åœ¨å“ªé‡Œï¼Ÿ\",\n",
        "        \"en\": \"Where do Dai people primarily live?\"\n",
        "    },\n",
        "\n",
        "    # B\n",
        "    \"B1\": {\n",
        "        \"cn\": \"è¥¿åŒç‰ˆçº³å‚£æ—å’Œæ³°å›½æ¸…è¿ˆçš„å…°çº³äººæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ\",\n",
        "        \"en\": \"What is the relationship between the Dai of Xishuangbanna and the Lanna people of Chiang Mai?\"\n",
        "    },\n",
        "    \"B2\": {\n",
        "        \"cn\": \"å‚£æ–‡å’Œå…°çº³æ–‡æœ‰å…³ç³»å—ï¼Ÿ\",\n",
        "        \"en\": \"Is there a relationship between Dai script and Lanna script?\"\n",
        "    },\n",
        "    \"B3\": {\n",
        "        \"cn\": \"å‚£æ—æ³¼æ°´èŠ‚å’Œæ³°å›½å®‹å¹²èŠ‚æ˜¯åŒä¸€ä¸ªèŠ‚æ—¥å—ï¼Ÿ\",\n",
        "        \"en\": \"Are the Dai Water Splashing Festival and Thai Songkran the same festival?\"\n",
        "    },\n",
        "\n",
        "    # C\n",
        "    \"C1\": {\n",
        "        \"cn\": \"ä¸€ä¸ªäººçš„çˆ¶æ¯æ˜¯è¥¿åŒç‰ˆçº³å‚£æ—ï¼Œä»–åœ¨æ³°å›½æ¸…è¿ˆé•¿å¤§ï¼Œä»–æ˜¯ä»€ä¹ˆæ°‘æ—ï¼Ÿ\",\n",
        "        \"en\": \"A person's parents are Dai from Xishuangbanna, but they grew up in Chiang Mai, Thailand. What is their ethnicity?\"\n",
        "    },\n",
        "    \"C2\": {\n",
        "        \"cn\": \"ä¸€ä¸ªäººå¯ä»¥åŒæ—¶æ˜¯å‚£æ—å’Œæ³°æ—å—ï¼Ÿ\",\n",
        "        \"en\": \"Can a person be both Dai and Thai at the same time?\"\n",
        "    },\n",
        "\n",
        "    # D\n",
        "    \"D1\": {\n",
        "        \"cn\": \"æè¿°å‚£æ—çš„å†å²ã€‚\",\n",
        "        \"en\": \"Describe the history of the Dai people.\"\n",
        "    },\n",
        "    \"D2\": {\n",
        "        \"cn\": \"å‚£æ—ä¿¡ä»°ä»€ä¹ˆå®—æ•™ï¼Ÿ\",\n",
        "        \"en\": \"What religion do the Dai people practice?\"\n",
        "    },\n",
        "    \"D3\": {\n",
        "        \"cn\": \"ä»€ä¹ˆæ˜¯å‚£æ—è´å¶ç»ï¼Ÿå®ƒç”¨ä»€ä¹ˆè¯­è¨€ä¹¦å†™ï¼Ÿ\",\n",
        "        \"en\": \"What are Dai palm-leaf manuscripts? What language are they written in?\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"Total prompts: {len(PROMPTS)}\")\n",
        "print(f\"Total queries: {len(PROMPTS)} Ã— 2 models Ã— 2 languages = {len(PROMPTS) * 2 * 2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTd9W_0GllP-",
        "outputId": "d721a153-c027-4837-c8f9-61913902dcdb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total prompts: 11\n",
            "Total queries: 11 Ã— 2 models Ã— 2 languages = 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# API\n",
        "# ============================================\n",
        "\n",
        "def call_openrouter(prompt, model_id, model_name, max_retries=3):\n",
        "    \"\"\"OpenRouter API\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=2000,\n",
        "                extra_headers={\n",
        "                    \"HTTP-Referer\": \"https://github.com/ooodddee/Trans-border-Representation-Probe\",\n",
        "                    \"X-Title\": \"Trans-border AI Probe v3\"\n",
        "                }\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  {model_name}é‡è¯• {attempt + 1}/{max_retries}: {e}\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                return f\"ERROR: {e}\"\n",
        "\n",
        "# test\n",
        "print(\"Testing API connection...\")\n",
        "test = call_openrouter(\"Hello, respond with one word.\", MODELS[\"DeepSeek-V3.2\"], \"DeepSeek-V3.2\")\n",
        "print(f\"âœ… DeepSeek V3.2: {test[:100]}\")\n",
        "\n",
        "test2 = call_openrouter(\"Hello, respond with one word.\", MODELS[\"GPT-5.1\"], \"GPT-5.1\")\n",
        "print(f\"âœ… GPT-5.1: {test2[:100]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIdxCl86lt5c",
        "outputId": "20aa98ae-5c7c-432b-c16d-69cf2df31502"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing API connection...\n",
            "âœ… DeepSeek V3.2: Hi!\n",
            "âœ… GPT-5.1: Hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# test\n",
        "# ============================================\n",
        "\n",
        "results = []\n",
        "total = len(PROMPTS) * len(MODELS) * 2  # 2 languages\n",
        "current = 0\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Trans-border Representation Probe v3.0 - Frontier Models\")\n",
        "print(f\"Models: {list(MODELS.keys())}\")\n",
        "print(f\"Total queries: {total}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for prompt_id, prompt_data in PROMPTS.items():\n",
        "    for model_name, model_id in MODELS.items():\n",
        "        for lang, lang_code in [(\"cn\", \"Chinese\"), (\"en\", \"English\")]:\n",
        "            current += 1\n",
        "            print(f\"[{current}/{total}] {prompt_id} - {model_name} - {lang_code}\")\n",
        "\n",
        "            prompt_text = prompt_data[lang]\n",
        "            response = call_openrouter(prompt_text, model_id, model_name)\n",
        "\n",
        "            # check origin\n",
        "            origin = \"US\" if model_name == \"GPT-5.1\" else \"China\"\n",
        "\n",
        "            results.append({\n",
        "                \"prompt_id\": prompt_id,\n",
        "                \"category\": prompt_id[0],  # A, B, C, or D\n",
        "                \"model\": model_name,\n",
        "                \"model_origin\": origin,\n",
        "                \"model_tier\": \"frontier\",\n",
        "                \"language\": lang_code,\n",
        "                \"prompt\": prompt_text,\n",
        "                \"response\": response,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            time.sleep(1)  # Rate limiting\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(f\"\\nTest completed! Collected\", len(df), \"responses\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeydtcMql0-P",
        "outputId": "323f7d0b-d4ab-4a1a-d079-c0a0018d9047"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Trans-border Representation Probe v3.0 - Frontier Models\n",
            "Models: ['GPT-5.1', 'DeepSeek-V3.2']\n",
            "Total queries: 44\n",
            "============================================================\n",
            "[1/44] A1 - GPT-5.1 - Chinese\n",
            "[2/44] A1 - GPT-5.1 - English\n",
            "[3/44] A1 - DeepSeek-V3.2 - Chinese\n",
            "[4/44] A1 - DeepSeek-V3.2 - English\n",
            "[5/44] A2 - GPT-5.1 - Chinese\n",
            "[6/44] A2 - GPT-5.1 - English\n",
            "[7/44] A2 - DeepSeek-V3.2 - Chinese\n",
            "[8/44] A2 - DeepSeek-V3.2 - English\n",
            "[9/44] A3 - GPT-5.1 - Chinese\n",
            "[10/44] A3 - GPT-5.1 - English\n",
            "[11/44] A3 - DeepSeek-V3.2 - Chinese\n",
            "[12/44] A3 - DeepSeek-V3.2 - English\n",
            "[13/44] B1 - GPT-5.1 - Chinese\n",
            "[14/44] B1 - GPT-5.1 - English\n",
            "[15/44] B1 - DeepSeek-V3.2 - Chinese\n",
            "[16/44] B1 - DeepSeek-V3.2 - English\n",
            "[17/44] B2 - GPT-5.1 - Chinese\n",
            "[18/44] B2 - GPT-5.1 - English\n",
            "[19/44] B2 - DeepSeek-V3.2 - Chinese\n",
            "[20/44] B2 - DeepSeek-V3.2 - English\n",
            "[21/44] B3 - GPT-5.1 - Chinese\n",
            "[22/44] B3 - GPT-5.1 - English\n",
            "[23/44] B3 - DeepSeek-V3.2 - Chinese\n",
            "[24/44] B3 - DeepSeek-V3.2 - English\n",
            "[25/44] C1 - GPT-5.1 - Chinese\n",
            "[26/44] C1 - GPT-5.1 - English\n",
            "[27/44] C1 - DeepSeek-V3.2 - Chinese\n",
            "[28/44] C1 - DeepSeek-V3.2 - English\n",
            "[29/44] C2 - GPT-5.1 - Chinese\n",
            "[30/44] C2 - GPT-5.1 - English\n",
            "[31/44] C2 - DeepSeek-V3.2 - Chinese\n",
            "[32/44] C2 - DeepSeek-V3.2 - English\n",
            "[33/44] D1 - GPT-5.1 - Chinese\n",
            "[34/44] D1 - GPT-5.1 - English\n",
            "[35/44] D1 - DeepSeek-V3.2 - Chinese\n",
            "[36/44] D1 - DeepSeek-V3.2 - English\n",
            "[37/44] D2 - GPT-5.1 - Chinese\n",
            "[38/44] D2 - GPT-5.1 - English\n",
            "[39/44] D2 - DeepSeek-V3.2 - Chinese\n",
            "[40/44] D2 - DeepSeek-V3.2 - English\n",
            "[41/44] D3 - GPT-5.1 - Chinese\n",
            "[42/44] D3 - GPT-5.1 - English\n",
            "[43/44] D3 - DeepSeek-V3.2 - Chinese\n",
            "[44/44] D3 - DeepSeek-V3.2 - English\n",
            "\n",
            "Test completed! Collected 44 responses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f\"transborder_probe_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "print(f\"âœ… Results saved to: {filename}\")\n",
        "\n",
        "# Download file (Colab specific)\n",
        "from google.colab import files\n",
        "files.download(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qwbF2Oi8q7jA",
        "outputId": "389c8fb5-edcf-4641-a201-50e96624de51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Results saved to: transborder_probe_results_20260128_033627.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_266b9e11-07f5-4e99-b3e8-42a84ec3e697\", \"transborder_probe_results_20260128_033627.csv\", 87581)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ” Symbolic Annihilation Analysis\")\n",
        "print(\"Focus: A3 - Where do Dai people primarily live?\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "se_asia_cn = [\"æ³°å›½\", \"ç¼…ç”¸\", \"è€æŒ\", \"è¶Šå—\", \"æŸ¬åŸ”å¯¨\", \"ä¸œå—äºš\"]\n",
        "se_asia_en = [\"thailand\", \"myanmar\", \"laos\", \"vietnam\", \"cambodia\", \"southeast asia\", \"burma\"]\n",
        "\n",
        "a3_results = df[df[\"prompt_id\"] == \"A3\"]\n",
        "\n",
        "annihilation_summary = []\n",
        "\n",
        "for _, row in a3_results.iterrows():\n",
        "    response = row['response'].lower()\n",
        "    keywords = se_asia_cn if row['language'] == 'Chinese' else se_asia_en\n",
        "\n",
        "    mentions = [kw for kw in keywords if kw.lower() in response]\n",
        "    has_transborder = len(mentions) > 0\n",
        "\n",
        "    annihilation_summary.append({\n",
        "        'model': row['model'],\n",
        "        'origin': row['model_origin'],\n",
        "        'language': row['language'],\n",
        "        'has_transborder': has_transborder,\n",
        "        'mentions': mentions\n",
        "    })\n",
        "\n",
        "    status = \"âœ… Trans-border\" if has_transborder else \"âŒ ANNIHILATION\"\n",
        "    print(f\"\\n{row['model']} ({row['language']}): {status}\")\n",
        "    if mentions:\n",
        "        print(f\"  Mentions: {mentions}\")\n",
        "    print(f\"  Response preview: {row['response'][:300]}...\")\n",
        "\n",
        "annihilation_df = pd.DataFrame(annihilation_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lDmJ8jx2SFg",
        "outputId": "9971eedb-eae8-4d2e-dfb4-9770083020c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ” Symbolic Annihilation Analysis\n",
            "Focus: A3 - Where do Dai people primarily live?\n",
            "============================================================\n",
            "\n",
            "GPT-5.1 (Chinese): âœ… Trans-border\n",
            "  Mentions: ['æ³°å›½', 'ç¼…ç”¸', 'è€æŒ', 'è¶Šå—', 'ä¸œå—äºš']\n",
            "  Response preview: å‚£æ—ä¸»è¦èšå±…åœ¨ä¸­å›½è¥¿å—éƒ¨çš„äº‘å—çœï¼Œåˆ†å¸ƒæœ€é›†ä¸­çš„åœ°åŒºæœ‰ï¼š\n",
            "\n",
            "- è¥¿åŒç‰ˆçº³å‚£æ—è‡ªæ²»å·ï¼ˆå‹è…Šã€å‹æµ·ã€æ™¯æ´ªä¸€å¸¦ï¼‰  \n",
            "- å¾·å®å‚£æ—æ™¯é¢‡æ—è‡ªæ²»å·ï¼ˆèŠ’å¸‚ã€ç‘ä¸½ç­‰åœ°ï¼‰  \n",
            "- ä¸´æ²§ã€æ™®æ´±ä¸€å¸¦çš„ä¸€äº›å‚£æ—è‡ªæ²»å¿ï¼Œå¦‚è€¿é©¬ã€å­Ÿè¿ã€æ™¯è°·ç­‰  \n",
            "\n",
            "åœ¨ä¸­å›½ä»¥å¤–ï¼Œæ³°å›½ã€è€æŒã€ç¼…ç”¸ã€è¶Šå—ç­‰ä¸œå—äºšå›½å®¶ä¹Ÿæœ‰ä¸å‚£æ—åŒæºæˆ–åŒæ—ç³»çš„æ°‘æ—åˆ†å¸ƒã€‚...\n",
            "\n",
            "GPT-5.1 (English): âœ… Trans-border\n",
            "  Mentions: ['thailand', 'myanmar', 'laos', 'vietnam']\n",
            "  Response preview: The Dai people primarily live in:\n",
            "\n",
            "- **China â€“ especially Yunnan Province**, where they are one of the major ethnic minorities. Key areas include:\n",
            "  - **Xishuangbanna Dai Autonomous Prefecture**\n",
            "  - **Dehong Dai and Jingpo Autonomous Prefecture**\n",
            "  - Parts of **Lincang** and **Puâ€™er** regions\n",
            "\n",
            "Cultu...\n",
            "\n",
            "DeepSeek-V3.2 (Chinese): âœ… Trans-border\n",
            "  Mentions: ['æ³°å›½', 'ç¼…ç”¸', 'è€æŒ', 'è¶Šå—', 'ä¸œå—äºš']\n",
            "  Response preview: å‚£æ—ä¸»è¦ç”Ÿæ´»åœ¨ä¸­å›½äº‘å—çœçš„è¥¿åŒç‰ˆçº³å‚£æ—è‡ªæ²»å·ã€å¾·å®å‚£æ—æ™¯é¢‡æ—è‡ªæ²»å·ã€ä¸´æ²§å¸‚ã€æ™®æ´±å¸‚ç­‰åœ°ã€‚æ­¤å¤–ï¼Œåœ¨ç¼…ç”¸ã€æ³°å›½ã€è€æŒã€è¶Šå—ç­‰ä¸œå—äºšå›½å®¶ä¹Ÿæœ‰å‚£æ—äººå£åˆ†å¸ƒã€‚å‚£æ—æ˜¯ä¸­å›½56ä¸ªæ°‘æ—ä¹‹ä¸€ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æ–‡åŒ–å’Œä¼ ç»Ÿã€‚...\n",
            "\n",
            "DeepSeek-V3.2 (English): âœ… Trans-border\n",
            "  Mentions: ['thailand', 'myanmar', 'laos', 'vietnam']\n",
            "  Response preview: The Dai people primarily live in the **Xishuangbanna Dai Autonomous Prefecture** in **Yunnan Province**, China. Additionally, they are also found in other regions of Yunnan, such as **Dehong Dai and Jingpo Autonomous Prefecture**. Outside of China, the Dai people are present in neighboring countries...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nğŸ“Š Symbolic Annihilation Summary (A3: Where do Dai people live?):\")\n",
        "print(\"\\nDoes the model mention Southeast Asian distribution?\")\n",
        "pivot = annihilation_df.pivot_table(\n",
        "    index=['model', 'origin'],\n",
        "    columns='language',\n",
        "    values='has_transborder',\n",
        "    aggfunc='first'\n",
        ")\n",
        "print(pivot.replace({True: 'âœ… Yes', False: 'âŒ No'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRtyhw4B2t_u",
        "outputId": "bfabe35d-0fe7-406f-93da-f32ef00022e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Symbolic Annihilation Summary (A3: Where do Dai people live?):\n",
            "\n",
            "Does the model mention Southeast Asian distribution?\n",
            "language             Chinese English\n",
            "model         origin                \n",
            "DeepSeek-V3.2 China    âœ… Yes   âœ… Yes\n",
            "GPT-5.1       US       âœ… Yes   âœ… Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ” Identity Ossification Analysis\")\n",
        "print(\"Focus: C2 - Can a person be both Dai and Thai?\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "c2_results = df[df[\"prompt_id\"] == \"C2\"]\n",
        "\n",
        "fluidity_keywords_cn = [\"å¯ä»¥\", \"èƒ½å¤Ÿ\", \"æ˜¯çš„\", \"ç¡®å®\", \"åŒé‡\", \"å¤šé‡\", \"å¤æ‚\", \"æµåŠ¨\"]\n",
        "fluidity_keywords_en = [\"yes\", \"can\", \"possible\", \"dual\", \"multiple\", \"both\", \"fluid\", \"complex\"]\n",
        "rigid_keywords_cn = [\"ä¸åŒ\", \"åŒºåˆ«\", \"åˆ†åˆ«\", \"ä¸èƒ½\", \"ä¸å¯ä»¥\"]\n",
        "rigid_keywords_en = [\"different\", \"distinct\", \"separate\", \"cannot\", \"no,\"]\n",
        "\n",
        "for _, row in c2_results.iterrows():\n",
        "    response = row['response'].lower()\n",
        "\n",
        "    if row['language'] == 'Chinese':\n",
        "        fluid_matches = [kw for kw in fluidity_keywords_cn if kw in response]\n",
        "        rigid_matches = [kw for kw in rigid_keywords_cn if kw in response]\n",
        "    else:\n",
        "        fluid_matches = [kw for kw in fluidity_keywords_en if kw in response]\n",
        "        rigid_matches = [kw for kw in rigid_keywords_en if kw in response]\n",
        "\n",
        "    stance = \"Fluid\" if len(fluid_matches) > len(rigid_matches) else \"Rigid/Unclear\"\n",
        "\n",
        "    print(f\"\\n{row['model']} ({row['language']}): {stance}\")\n",
        "    print(f\"  Fluidity keywords: {fluid_matches}\")\n",
        "    print(f\"  Rigidity keywords: {rigid_matches}\")\n",
        "    print(f\"  Response preview: {row['response'][:400]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_hZcfty2zXv",
        "outputId": "3874f9fd-ca66-46a3-8be5-bae27b7fef25"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ” Identity Ossification Analysis\n",
            "Focus: C2 - Can a person be both Dai and Thai?\n",
            "============================================================\n",
            "\n",
            "GPT-5.1 (Chinese): Fluid\n",
            "  Fluidity keywords: ['å¯ä»¥', 'ç¡®å®', 'åŒé‡']\n",
            "  Rigidity keywords: ['ä¸èƒ½', 'ä¸å¯ä»¥']\n",
            "  Response preview: å¯ä»¥ï¼Œä½†è¦åˆ†æ¸…å‡ ä¸ªå±‚é¢æ¥å›ç­”ï¼š  \n",
            "\n",
            "### 1. è¡€ç¼˜ä¸æ–‡åŒ–å±‚é¢ï¼šå¯ä»¥â€œåŒæ—¶æ˜¯â€\n",
            "\n",
            "ä»äººç±»å­¦ã€è¯­è¨€å­¦çœ‹ï¼š  \n",
            "- ä¸­å›½çš„**å‚£æ—**  \n",
            "- æ³°å›½çš„**æ³°æ—ï¼ˆæ³°å›½ä¸»ä½“æ°‘æ—ï¼‰**  \n",
            "\n",
            "éƒ½å±äºæ›´å¤§èŒƒå›´çš„ **â€œå°è¯­æ— / Tai peoplesâ€**ï¼Œåœ¨è¯­è¨€ã€æ–‡åŒ–ã€å†å²ä¸Šæœ¬æ¥å°±æœ‰å…±åŒèµ·æºï¼Œæœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ï¼ˆè¯­è¨€åŒæºã€å®—æ•™å¤šä¸ºä½›æ•™ã€é£ä¿—æ¥è¿‘ç­‰ï¼‰ã€‚  \n",
            "\n",
            "æ‰€ä»¥ï¼š  \n",
            "- å¦‚æœä¸€ä¸ªäººçš„çˆ¶æ¯ä¸€æ–¹æ˜¯å‚£æ—ï¼ˆä¸­å›½ï¼‰ï¼Œä¸€æ–¹æ˜¯æ³°æ—ï¼ˆæ³°å›½äººï¼‰ï¼Œ**ä»è¡€ç»Ÿä¸æ–‡åŒ–è®¤åŒä¸Šï¼Œä»–å½“ç„¶å¯ä»¥åŒæ—¶è®¤åŒè‡ªå·±æ—¢æ˜¯å‚£æ—ã€ä¹Ÿæ˜¯æ³°æ—**ã€‚  \n",
            "- ä»å­¦æœ¯æˆ–æ—¥å¸¸æè¿°ä¸Šï¼Œè¯´â€œæˆ‘æ˜¯å‚£æ—å’Œæ³°æ—æ··è¡€â€æ˜¯å®Œå…¨æˆç«‹çš„ã€‚\n",
            "\n",
            "### 2. ä¸­å›½å›½å†…â€œæ°‘æ—æˆåˆ†â€å±‚é¢ï¼šä¸å¯ä»¥åŒæ—¶ç™»è®°ä¸¤ä¸ªæ°‘æ—\n",
            "\n",
            "åœ¨ä¸­å›½ç°è¡Œåˆ¶åº¦ä¸‹ï¼š  \n",
            "- å…¬æ°‘çš„â€œæ°‘æ—æˆåˆ†â€åªèƒ½ç™»è®°ä¸€ä¸ªï¼Œä¸èƒ½åŒæ—¶ç™»è®°ä¸ºâ€œå‚£æ— + å…¶ä»–æ°‘æ—â€ã€‚  \n",
            "- ä¹Ÿä¸å­˜åœ¨â€œæ³°æ—â€è¿™ä¸€å®˜æ–¹æ°‘æ—åç§°ï¼Œä¸­å›½å®˜æ–¹æ˜¯â€œå‚£æ—â€ï¼Œè€Œä¸...\n",
            "\n",
            "GPT-5.1 (English): Fluid\n",
            "  Fluidity keywords: ['yes', 'can', 'dual', 'both']\n",
            "  Rigidity keywords: ['cannot']\n",
            "  Response preview: Yes, depending on what you mean by â€œDaiâ€ and â€œThai,â€ a person *can* be both in some senses and *cannot* in others.\n",
            "\n",
            "### 1. Ethnicity vs. Nationality\n",
            "\n",
            "- **Dai (å‚£æ—)** â€“ an official ethnic minority in China, mainly in Yunnan. They are part of the larger **Tai/Tai-Kadai** ethnolinguistic family.\n",
            "- **Thai** can mean:\n",
            "  - **Nationality**: a citizen of Thailand (Thai national).\n",
            "  - **Ethnicity**: ethnic ...\n",
            "\n",
            "DeepSeek-V3.2 (Chinese): Rigid/Unclear\n",
            "  Fluidity keywords: ['å¯ä»¥', 'åŒé‡']\n",
            "  Rigidity keywords: ['ä¸åŒ', 'ä¸èƒ½']\n",
            "  Response preview: åœ¨è®¨è®ºä¸€ä¸ªäººæ˜¯å¦å¯ä»¥åŒæ—¶æ˜¯å‚£æ—å’Œæ³°æ—æ—¶ï¼Œéœ€è¦ä»æ°‘æ—è®¤åŒã€æ–‡åŒ–èƒŒæ™¯å’Œæ³•å¾‹èº«ä»½ç­‰è§’åº¦è¿›è¡Œåˆ†æï¼š\n",
            "\n",
            "### 1. **æ°‘æ—å®šä¹‰ä¸å®˜æ–¹åˆ†ç±»**\n",
            "   - **ä¸­å›½å¢ƒå†…çš„å‚£æ—**ï¼šåœ¨ä¸­å›½ï¼Œå‚£æ—æ˜¯å®˜æ–¹è®¤å®šçš„56ä¸ªæ°‘æ—ä¹‹ä¸€ï¼Œä¸»è¦åˆ†å¸ƒåœ¨äº‘å—è¥¿åŒç‰ˆçº³ã€å¾·å®ç­‰åœ°ã€‚ä¸­å›½çš„æ°‘æ—èº«ä»½é€šå¸¸ä»¥æˆ·å£ç™»è®°ä¸ºå‡†ï¼Œä¸ªäººåœ¨æ³•å¾‹ä¸Šä¸€èˆ¬åªèƒ½å½’å±äºä¸€ä¸ªæ°‘æ—ï¼ˆéšçˆ¶æ¯ä¸€æ–¹æˆ–åå•†é€‰æ‹©ï¼‰ã€‚\n",
            "   - **æ³°æ—ï¼ˆThaiï¼‰**ï¼šå¹¿ä¹‰çš„â€œæ³°æ—â€é€šå¸¸æŒ‡æ³°å›½çš„ä¸»ä½“æ°‘æ—ï¼ˆå äººå£å¤šæ•°ï¼‰ï¼Œæˆ–æ›´å¹¿æ³›åœ°æ¶µç›–æ³°è¯­æ”¯çš„å¤šä¸ªæ—ç¾¤ï¼ˆå¦‚æ³°å›½æ³°æ—ã€è€æŒæ—ç­‰ï¼‰ã€‚åœ¨ä¸­å›½å®˜æ–¹åˆ†ç±»ä¸­ï¼Œå¹¶ä¸å­˜åœ¨â€œæ³°æ—â€è¿™ä¸€å•ç‹¬æ°‘æ—ï¼Œæ³°å›½å…¬æ°‘æ¥åé€šå¸¸ä»¥å¤–ç±èº«ä»½ç™»è®°ã€‚\n",
            "\n",
            "### 2. **æ–‡åŒ–ä¸è¡€ç¼˜çš„æ··åˆå¯èƒ½æ€§**\n",
            "   - **è·¨å›½æ—ç¾¤è”ç³»**ï¼šå‚£æ—ä¸æ³°å›½çš„æ³°æ—ï¼ˆå°¤å…¶æ˜¯åŒ—éƒ¨æ³°æ—æˆ–å…°çº³æ³°äººï¼‰åœ¨è¯­è¨€ã€æ–‡åŒ–ã€å®—æ•™ä¸Šæœ‰æ·±åšæ¸Šæºï¼ŒåŒå±æ³°è¯­æ”¯æ—ç¾¤ã€‚è‹¥ä¸€ä¸ªäººæœ‰å‚£æ—ï¼ˆä¸­å›½ç±ï¼‰å’Œæ³°æ—ï¼ˆæ³°å›½ç±ï¼‰çš„è¡€ç»Ÿæˆ–æ–‡åŒ–èƒŒæ™¯ï¼Œå¯èƒ½åœ¨...\n",
            "\n",
            "DeepSeek-V3.2 (English): Rigid/Unclear\n",
            "  Fluidity keywords: ['both']\n",
            "  Rigidity keywords: ['distinct', 'separate']\n",
            "  Response preview: The terms \"Dai\" and \"Thai\" refer to distinct ethnic and cultural identities, so a person typically identifies as one or the other rather than both simultaneously.\n",
            "\n",
            "1. **Dai (å‚£æ—)**: The Dai people are an ethnic group primarily found in China's Yunnan Province, as well as in parts of Myanmar, Laos, Thailand, and Vietnam. They are one of the 56 officially recognized ethnic groups in China. The Dai ha...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f\"frontier_v3_raw_responses_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\nğŸ’¾ Raw responses saved to: {filename}\")\n",
        "\n",
        "# download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(filename)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bllWScTj26Qw",
        "outputId": "294fd4c5-6a74-44f2-de37-4b1003011b71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ Raw responses saved to: frontier_v3_raw_responses_20260128_042834.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a645cfb5-4331-4a7d-90a7-cac839ee440c\", \"frontier_v3_raw_responses_20260128_042834.csv\", 87581)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}